{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copie de Phrase_level_classification_1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306e663e",
        "outputId": "e9b557ea-164d-43bd-ab87-122edbd297fb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from gensim.models import word2vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "!python -m spacy download fr_core_news_md"
      ],
      "id": "306e663e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr_core_news_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 88.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-py3-none-any.whl size=90338488 sha256=d1f6fab7315c40084ef9d6670947f41a6b0284b3b41d0d1cc02e3cc665a24284\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e03sh4o3/wheels/2e/26/ff/ce93eb966e7176ebe81e6c98209582e13e108cdd2d6d636df0\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "4da26dcb",
        "outputId": "6ec06f48-06d9-477f-93ce-d5ea0e55af11"
      },
      "source": [
        "data = pd.read_csv('training_data.csv')\n",
        "data.drop(columns = ['id'], inplace = True)\n",
        "data.head()"
      ],
      "id": "4da26dcb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f2acc1892c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abd18a35"
      },
      "source": [
        "def type_counter(df):\n",
        "    nlp = spacy.load('fr_core_news_md')\n",
        "    dif_typ = []\n",
        "    \n",
        "    for i in range(df.shape[0]):\n",
        "        doc = nlp(df['sentence'].iloc[i])\n",
        "        for j in doc:\n",
        "            if j.pos_ not in dif_typ:\n",
        "                dif_typ.append(j.pos_)\n",
        "                \n",
        "    res = []\n",
        "    \n",
        "    for i in range(df.shape[0]):\n",
        "        temp = np.zeros(len(dif_typ))\n",
        "        doc = nlp(df['sentence'].iloc[i])\n",
        "        for j in doc:\n",
        "            ind = dif_typ.index(j.pos_)\n",
        "            temp[ind] += 1\n",
        "        res.append(temp)\n",
        "        \n",
        "                \n",
        "    return res"
      ],
      "id": "abd18a35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8f8334"
      },
      "source": [
        "def sk_vec(df):\n",
        "    phrases = df['sentence'].tolist()\n",
        "    vectorizer = CountVectorizer()\n",
        "    return vectorizer.fit_transform(phrases).toarray()"
      ],
      "id": "4b8f8334",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67936ab7"
      },
      "source": [
        "def vectorizer(df):\n",
        "    tokenized_sentence = []\n",
        "    nlp = spacy.load('fr_core_news_md')\n",
        "    for sentence in df['sentence'].tolist():\n",
        "        doc = nlp(sentence)\n",
        "        tokenized_sentence.append(doc.vector)\n",
        "        \n",
        "    return tokenized_sentence            "
      ],
      "id": "67936ab7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8735f35"
      },
      "source": [
        "def word_len_counter(df):\n",
        "    res = []\n",
        "    nlp = spacy.load('fr_core_news_md')\n",
        "    for sentence in df['sentence'].tolist():\n",
        "        temp = []\n",
        "        doc = nlp(sentence)\n",
        "        #doc=nb mots dans une phrase (ponct comprise)\n",
        "        temp.append(len(doc))\n",
        "        #sentence = nb de caractères dans une phrase(ponct comprise)\n",
        "        temp.append(len(sentence))\n",
        "        res.append(temp)\n",
        "    return res\n",
        "        "
      ],
      "id": "f8735f35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21426b4b"
      },
      "source": [
        "def prepare_output(df):\n",
        "    res1 = df['difficulty'].map({'A1':0, 'A2':1, 'B1':2,'B2':3,'C1':4,'C2':5}).tolist()\n",
        "    \n",
        "    res2 = []\n",
        "    mapping =  {'A1':0, 'A2':1, 'B1':2,'B2':3,'C1':4,'C2':5}\n",
        "    for i in range(len(df)):\n",
        "        temp = np.zeros(6)\n",
        "        temp[mapping[df['difficulty'].iloc[i]]] = 1\n",
        "        res2.append(temp)\n",
        "        \n",
        "    \n",
        "    return res1, res2"
      ],
      "id": "21426b4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9295f3b"
      },
      "source": [
        "def tf_accuracy(model, X,y):\n",
        "    pred = model.predict(X)\n",
        "    correct = 0\n",
        "    for i in range(len(pred)):\n",
        "        val =np.argmax(pred[i])\n",
        "        if y[i][val] == 1:\n",
        "            correct +=1\n",
        "    return correct/len(y)"
      ],
      "id": "e9295f3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cbe29c6"
      },
      "source": [
        "feature1 = type_counter(data)"
      ],
      "id": "0cbe29c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8716b55b"
      },
      "source": [
        "feature2 = vectorizer(data)"
      ],
      "id": "8716b55b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1750f4ab"
      },
      "source": [
        "feature3 = word_len_counter(data)"
      ],
      "id": "1750f4ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "936ca65c"
      },
      "source": [
        "feature4 = sk_vec(data)"
      ],
      "id": "936ca65c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33c150d3"
      },
      "source": [
        "feature = np.concatenate((feature1, feature2),axis = 1)\n",
        "feature = np.concatenate((feature, feature3), axis = 1)\n",
        "feature = np.concatenate((feature, feature4), axis = 1)"
      ],
      "id": "33c150d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fc84c0f"
      },
      "source": [
        "y1, y2 = prepare_output(data)"
      ],
      "id": "1fc84c0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI9ncD3gssic"
      },
      "source": [
        "print(y1, y2)"
      ],
      "id": "fI9ncD3gssic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ac5da43"
      },
      "source": [
        "#X_train\n",
        "feature1_train = np.array(feature[:4000])\n",
        "y2_train = np.asarray(y2[:4000])\n",
        "y1_train = np.asarray(y1[:4000])\n",
        "#X_test\n",
        "feature1_test = np.array(feature[4000:])\n",
        "y2_test = np.asarray(y2[4000:])\n",
        "y1_test = np.asarray(y1[4000:])"
      ],
      "id": "2ac5da43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61f67df4"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(input_shape = feature1_train[0].shape, units=512, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=6, activation='softmax')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "id": "61f67df4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c969de9f"
      },
      "source": [
        "model.compile(optimizer='adamax', \n",
        "              loss=tf.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "id": "c969de9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce02ccc1"
      },
      "source": [
        "model.fit(feature1_train, y2_train, epochs = 100, steps_per_epoch = 500, validation_split=0.01)"
      ],
      "id": "ce02ccc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5754d5d2"
      },
      "source": [
        "tf_accuracy(model, feature1_test, y2_test)"
      ],
      "id": "5754d5d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml6-SwBkwvo3"
      },
      "source": [
        ""
      ],
      "id": "Ml6-SwBkwvo3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9bJ4_8bwEeC"
      },
      "source": [
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average=\"weighted\")\n",
        "    recall = recall_score(true, pred, average=\"weighted\")\n",
        "    f1 = f1_score(true, pred, average=\"weighted\")\n",
        "    print(confusion_matrix(true, pred))\n",
        "    print(accuracy_score(true, pred))\n",
        "    print(precision, recall, f1)"
      ],
      "id": "u9bJ4_8bwEeC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11QYtnkeyan1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier=classifier.fit(feature1_train, y1_train)\n",
        "\n",
        "\n",
        "y_pred=classifier.predict(feature1_test)\n",
        "for i in y_pred:\n",
        "  temp=None\n",
        "  \n",
        "\n",
        "evaluate(y1_test, y_pred)\n",
        "\n"
      ],
      "id": "11QYtnkeyan1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WolyrN0x8T-0"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "classifier = KNeighborsClassifier()\n",
        "classifier=classifier.fit(feature1_train, y1_train)\n",
        "\n",
        "\n",
        "y_pred=classifier.predict(feature1_test)\n",
        "  \n",
        "\n",
        "evaluate(y1_test, y_pred)\n",
        "\n"
      ],
      "id": "WolyrN0x8T-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5MSVZ4HEQoU"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier=classifier.fit(feature1_train, y1_train)\n",
        "\n",
        "\n",
        "y_pred=classifier.predict(feature1_test)\n",
        "  \n",
        "\n",
        "evaluate(y1_test, y_pred)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "id": "U5MSVZ4HEQoU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOTjehzsEgxr"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier()\n",
        "classifier=classifier.fit(feature1_train, y1_train)\n",
        "\n",
        "\n",
        "y_pred=classifier.predict(feature1_test)\n",
        "  \n",
        "\n",
        "evaluate(y1_test, y_pred)\n",
        "\n"
      ],
      "id": "hOTjehzsEgxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQsy5HAH1e7z"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(input_shape = feature1_train[0].shape, units=512, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=6, activation='softmax')\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "model.compile(optimizer='adamax', \n",
        "              loss=tf.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(np.asarray(feature), np.asarray(y2), epochs = 100, steps_per_epoch = 500, validation_split=0.01)"
      ],
      "id": "TQsy5HAH1e7z",
      "execution_count": null,
      "outputs": []
    }
  ]
}